{"cells":[{"cell_type":"markdown","metadata":{"id":"FB23C3A83BD146A58F3BB71296D00045","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["# Model-Free TD Control: Q-Learning and SARSA\n","\n","**本notebook包含2个习题(Exercise 1\\~2)和5个代码填空(Programming 1\\~5)。**\n","\n","\n","## 时间差分学习 Temporal Difference(TD)：\n","1. TD结合了Dynamic Programming(动态规划，下称DP)和 MonteCarlo方法(蒙特卡洛方法，下称MC)的思想。TD和MC的共同优势是可以从原始经验中学习，不需要知道环境模型；TD不需要仿真出完整的轨迹，直接利用其它状态的估计来更新当前状态值，这一点和DP方法很类似。\n","2. DP、MC、TD三者的具体方法都是广义策略迭代的思想，在预测阶段，即值函数评估，三种方法各不相同，DP使用Bellman方程，MC使用采样回报均值，TD则是通过对已计算出的结果进行更新。\n","\n","\n","## on-policy & off-policy\n","1. on-policy：生成样本的policy（value function）跟网络更新参数时使用的policy（value function）相同。典型为SARAS算法，基于当前的policy直接执行一次动作选择，然后用这个样本更新当前的policy，因此生成样本的policy和学习时的policy相同，算法为on-policy算法。该方法会遭遇探索-利用的矛盾，光利用目前已知的最优选择，可能学不到最优解，收敛到局部最优，而加入探索又降低了学习效率。epsilon-greedy 算法是这种矛盾下的折衷。优点是直接了当，速度快，劣势是不一定找到最优策略。\n","\n","2. off-policy：生成样本的policy（value function）跟网络更新参数时使用的policy（value function）不同。典型为Q-learning算法，计算下一状态的预期收益时使用了max操作，直接选择最优动作，而当前policy并不一定能选择到最优动作，因此这里生成样本的policy和学习时的policy不同，为off-policy算法。先产生某概率分布下的大量行为数据（behavior policy），意在探索。从这些偏离（off）最优策略的数据中寻求target policy。当然这么做是需要满足数学条件的：假設π是目标策略, µ是行为策略，那么从µ学到π的条件是：π(a|s) > 0 必然有 µ(a|s) > 0成立。\n","\n","3. 两种学习策略的关系是：on-policy是off-policy 的特殊情形，其target policy 和behavior policy是一个。劣势是曲折，收敛慢，但优势是更为强大和通用。其强大是因为它确保了数据全面性，所有行为都能覆盖。\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1DFD21A3032847B882971A6B91E5F741","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## 运行环境：Cliff Walking\n","\n","![Image Name](https://cdn.kesci.com/upload/image/q9v4d39ald.png)\n","这是一个标准的无折扣、分阶段、有起点和终点的模型，在每个位置通常有上下左右四种操作，在悬崖格子上的reward是-100，其他格子的reward是-1。而且如果走到了悬崖格子，会被直接传送到起点。"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"9929D333324342078BF6342FAB11C9B9","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["import time\n","import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","class Env():\n","    def __init__(self, length, height):\n","        self.length = length        #模型的长，我们采用12\n","        self.height = height        #模型的宽，我们采用4\n","        self.x = 0                  #记录当前位置横坐标\n","        self.y = 0                  #记录当前位置纵坐标\n","\n","    def render(self, frames=50):\n","        for i in range(self.height):\n","            if i == 0: \n","                line = ['S'] + ['x']*(self.length - 2) + ['T'] #设置左下角位置为起点，右下角位置为重点\n","            else:\n","                line = ['.'] * self.length #普通的格子，走上去有-1的reward\n","            if self.x == i:\n","                line[self.y] = 'o' #墙壁所在位置如上图所示\n","            print(''.join(line))\n","        print('\\033['+str(self.height+1)+'A')  \n","        time.sleep(1.0 / frames)\n","\n","    def step(self, action): #外部调用这个函数来让当前位置改变\n","        \"\"\"4 legal actions, 0:up, 1:down, 2:left, 3:right\"\"\"\n","        change = [[0, 1], [0, -1], [-1, 0], [1, 0]]\n","        self.x = min(self.height - 1, max(0, self.x + change[action][0]))\n","        self.y = min(self.length - 1, max(0, self.y + change[action][1]))\n","\n","        states = [self.x, self.y]\n","        reward = -1\n","        terminal = False\n","        if self.x == 0: \n","            if self.y > 0:\n","                terminal = True\n","                if self.y != self.length - 1:\n","                    reward = -100\n","        return reward, states, terminal\n","\n","    def reset(self): #交互程序回归初始状态\n","        self.x = 0\n","        self.y = 0"]},{"cell_type":"markdown","metadata":{"id":"A448D880C7634E66853AAF7CEC0DC5A1","jupyter":{},"mdEditEnable":false,"notebookId":"604f2dd7df26380015a09910","slideshow":{"slide_type":"slide"},"tags":[]},"source":["基于该环境，先定义基本算法框架"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DE5FB75264746CE8EB8ED8DBB7DD0E8","jupyter":{},"notebookId":"604f2dd7df26380015a09910","scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["class Base_Q_table():\n","    def __init__(self, length, height, actions=4, alpha=0.1, gamma=0.9):\n","        self.table = [0] * actions * length * height # 初始化Q(s,a)这个表\n","        self.actions = actions\n","        self.length = length\n","        self.height = height\n","        self.alpha = alpha #学习率\n","        self.gamma = gamma #衰减参数\n","\n","    def _index(self, a, x, y):\n","        return a * self.height * self.length + x * self.length + y\n","\n","    def best_direction(self, x, y):\n","        mav = -100000\n","        mapos = -1\n","        for i in range(self.actions):\n","            if(self.table[self._index(i,x,y)] > mav):\n","                mav = self.table[self._index(i,x,y)]\n","                mapos = i\n","        return mapos\n","\n","    def _epsilon(self, num_episode): #我们这里采用了衰减的epsilon以获得相对优秀的收敛效果\n","        return min(0.5, 20. / (num_episode + 1))\n","\n","    def max_q(self, x, y):\n","        action = self.best_direction(x, y)\n","        return self.table[self._index(action,x,y)]\n","        \n","    def take_action(self, x, y, num_episode): #选取下一步的操作\n","        ########################################\n","        ## Programming 1: epsilon-greedy选取动作\n","        if np.random.random() < self._epsilon(num_episode):\n","            action = np.random.randint(self.actions)\n","        else:\n","            action = self.best_direction(x,y)\n","        ########################################\n","        return action\n","\n","    def update(self, direct, next_direct, s0, s1, reward, is_terminated):\n","        pass\n","        \n","    \n","    \n"]},{"cell_type":"markdown","metadata":{"id":"13C830FB004544F68B80FAFED7DB1BBE","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## SARSA\n","\n","算法介绍\n","1. 对于当前策略执行获得的每个sarsa五元组，其中s指状态，a指动作，r指回报，每次转移涉及的五个量构成了它的名称\n","2. SARSA是对状态-动作值函数进行更新\n","3. 是一种On-policy Control的方法\n","4. 是一种模型无关的方法\n","\n","使用SARSA的在线策略控制 on-policy\n","1. 策略评估：SARSA Q(s, a) ← Q(s, a) + α(r + γQ(s', a') - Q(s, a))\n","2. 策略改进：ϵ-greedy 策略改进\n","3. 因为on-policy，所以SARSA中两个A都是来自同一个策略（当前策略）\n"]},{"cell_type":"markdown","metadata":{"id":"40D864F5AB1346E4929F0D2226E748CB","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## SARSA实战代码"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7448457EEE784F869E2FD9409870739B","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["class Q_table_sarsa(Base_Q_table):\n","    def __init__(self, length, height, actions=4, alpha=0.1, gamma=0.9):\n","        super().__init__(length, height, actions, alpha, gamma)\n","\n","    def update(self, direct, next_direct, s0, s1, reward, is_terminated):\n","        ########################################\n","        ## Programming 2: 更新Q函数表self.table\n","        if is_terminated == False:\n","            td_error = reward + self.gamma * self.table[self._index(next_direct,s1[0],s1[1])] - self.table[self._index(direct,s0[0],s0[1])]\n","            self.table[self._index(direct,s0[0],s0[1])] += self.alpha * td_error\n","        else:\n","            td_error = reward - self.table[self._index(direct,s0[0],s0[1])]\n","            self.table[self._index(direct,s0[0],s0[1])] += self.alpha * td_error \n","        ########################################\n","\n","        \n","    \n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFE9845C56994EE4B238D3F7D26F5F5B","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["val = [0] * 150\n","x = [0] * 150\n","\n","\n","def cliff_walk_sarsa():\n","    env = Env(length=12, height=4)\n","    table = Q_table_sarsa(length=12, height=4)\n","    for num_episode in range(3000):\n","        episodic_reward = 0\n","        is_terminated = False\n","        s0 = [0, 0]\n","        action = table.take_action(s0[0], s0[1], num_episode)\n","        while not is_terminated:\n","\n","            reward, s1, is_terminated = env.step(action)\n","            next_action = table.take_action(s1[0], s1[1], num_episode)\n","\n","            episodic_reward += reward\n","\n","            table.update(action, next_action, s0, s1, reward, is_terminated)\n","\n","            s0 = s1\n","            action = next_action\n","\n","        if num_episode % 20 == 0:\n","            val[(int)(num_episode/20)] = episodic_reward\n","\n","        env.reset()\n","\n","\n","cliff_walk_sarsa()\n","\n","for i in range((int)(3000/20)):\n","    x[i] = i\n","plt.plot(x, val, ls=\"-\", lw=2, label=\"plot figure\")\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"69B05DCB36214B71AE8FD00238319C4D","jupyter":{},"mdEditEnable":false,"notebookId":"604f2dd7df26380015a09910","slideshow":{"slide_type":"slide"},"tags":[]},"source":["参考训练过程如下：\n","\n","![SARSA运行参考结果](https://cdn.kesci.com/upload/image/qq0aukv896.png)\n"]},{"cell_type":"markdown","metadata":{"id":"9DE8B248EFBA41B6A8E1211D397154B6","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## n-step SARSA\n","伪代码如下：\n","\n","![Image Name](https://cdn.kesci.com/upload/image/qqrs0vfv3u.png?imageView2/0/w/960/h/960)\n","\n","注意在遇到terminated state之后仍然有一段更新的过程。\n","\n","\n","类似SARSA算法，n-step算法就是每次并不是根据上一步的数据更新，而是根据之前n步的数据更新。下面以n=5为例子"]},{"cell_type":"markdown","metadata":{"id":"D14CB181112F4688AB87B48E748B7D84","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## n-step SARSA实战代码"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DB83C64EAFA42C495A35C2DF983A8C1","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["from collections import namedtuple\n","class Q_table_nstep_sarsa(Base_Q_table):\n","    def __init__(self, length, height, actions=4, alpha=0.02, gamma=0.9, table = [], n=5):\n","        # n: n-step 长度\n","        super().__init__(length, height, actions, alpha, gamma)\n","        self.n = n\n","        self.transition = namedtuple(\"transition\", [\"state\", \"action\", \"reward\"])\n","        self.trajectory = []  # 保存历史轨迹\n","\n","    def getval(self, tau, n):   \n","        ########################################\n","        ## Programming 3: 计算N-step Return $G_{tau:tau+n-1}$\n","        G = 0\n","        for i in range(tau, tau + n):\n","            G += np.power(self.gamma, i - tau - 1) * self.trajectory[i].reward\n","        ########################################\n","        return G\n","        \n","    def update(self, direct, next_direct, s0, s1, reward, is_terminated): \n","        #我们主要修改的点就在这里，每次保留n次的数据，从n次之前的数据来更新，可以看到收敛慢了一些，但是相对稳定。\n","        ########################################\n","        ## Programming 4: 更新Q函数表self.table\n","        self.trajectory.append(self.transition(s0, direct, reward))\n","        if len(self.trajectory) == self.n:\n","            G = reward + self.table[self._index(next_direct, s1[0], s1[1])]\n","            for i in reversed(range(self.n)):\n","                trans = self.trajectory[i]\n","                G = self.gamma * G + trans.reward\n","                if is_terminated and i > 0:\n","                    self.table[self._index(trans.action, trans.state[0], trans.state[1])] += self.alpha * (G - self.table[self._index(trans.action, trans.state[0], trans.state[1])])\n","            trans = self.trajectory.pop(0)\n","\n","            self.table[self._index(trans.action, trans.state[0], trans.state[1])] += self.alpha * (G - self.table[self._index(trans.action, trans.state[0], trans.state[1])])\n","        if is_terminated:\n","            self.trajectory = []\n","\n","        ########################################\n","            \n","\n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"512C6C1E774F4A2886730F01C024E248","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["val = [0] * 150\n","x = [0] * 150\n","def cliff_walk_nstep_sarsa():\n","    env = Env(length=12, height=4)\n","    table = Q_table_nstep_sarsa(length=12, height=4)\n","    for num_episode in range(3000):\n","        episodic_reward = 0\n","        is_terminated = False\n","        s0 = [0, 0]\n","        action = table.take_action(s0[0],s0[1],num_episode)\n","        while not is_terminated:\n","            \n","            reward, s1, is_terminated = env.step(action)\n","            next_action = table.take_action(s1[0],s1[1],num_episode)\n","            \n","            episodic_reward += reward\n","            \n","            table.update(action, next_action, s0, s1, reward, is_terminated)\n","            \n","            s0 = s1\n","            action = next_action\n","\n","        if num_episode % 20 == 0:\n","            val[(int)(num_episode/20)]=episodic_reward\n","        env.reset()\n","cliff_walk_nstep_sarsa()\n","for i in range((int)(3000/20)):\n","    x[i] = i\n","plt.plot(x, val, ls=\"-\", lw=2, label=\"plot figure\")\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"1EBC62E40F9F420F8E245613BAE098FB","jupyter":{},"mdEditEnable":false,"notebookId":"604f2dd7df26380015a09910","slideshow":{"slide_type":"slide"},"tags":[]},"source":["参考训练过程如下：\n","\n","![Image Name](https://cdn.kesci.com/upload/image/qqrql0yrn6.png?imageView2/0/w/960/h/960)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"37A050467BFC43058FAC45F732E668F4","jupyter":{},"mdEditEnable":false,"notebookId":"604f2dd7df26380015a09910","slideshow":{"slide_type":"slide"},"tags":[]},"source":["**Ex1 证明：**\n","在价值函数V不更新的假设下，\n","$$\n","G_{t: t+n}-V_{t+n-1}\\left(S_{t}\\right) =\\sum_{k=t}^{t+n-1} \\gamma^{k-t} \\delta_{k}~，\n","$$\n","其中$\\delta_{t} = R_{t+1}+\\gamma V_{t}\\left(S_{t+1}\\right)-V_{t}\\left(S_{t}\\right)$。\n","\n","Proof:\n","$$\n","\\text{Left} = G_{t: t+n}-V_{t+n-1}\\left(S_t\\right) = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^{n} Q(S_{t+n},A_{t+n}) - V_{t+n-1}(S_t)\n","$$\n","\n","$$\n","\\text{Right} = \\sum_{k=t}^{t+n-1}\\gamma^{k-t}\\delta_k = \\sum_{i=1}^{n}(\\gamma^{i-1}R_{t+i}+\\gamma^{i}V_{t+i-1}(S_{t+i})+\\gamma^{i-1}V_{t+i-1}(S_{t+i-1}))\n","$$\n","\n","Becouse we don't update $V$, which means $V_{t_1}(s)=V_{t_2}(s), \\quad\\forall t_1,t_2 \\leq T, \\quad\\forall s\\in\\mathcal{S}$. Subtract the dislocation of the last two items in the right formula, we have\n","\n","$$\n","\\text{Right} = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^{n}V_{t+n-1}(S_{t+n}) - V_t(S_t)\n","$$\n","\n","So\n","$$\n","V_t(S_t)= V_{t+n-1}(S_t),\\quad \\gamma^{n} Q(S_{t+n},A_{t+n})=\\gamma^{n}V_{t+n-1}(S_{t+n})\n","$$"]},{"cell_type":"markdown","metadata":{"id":"336D4C26ED9440A58A919481CE295EC6","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## Q-Learning Algorithm\n","\n","Q-learning算法是一种off-policy的时序差分控制算法, 是model-free RL中出现很早的一个模型, 模型如下\n","\n","$$\n","Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a\\in \\mathcal{A}}Q(s_{t+1},a_t) - Q(s_t,a_t) \\right]\n","$$\n","\n","\n","我们会使用到$\\epsilon$-贪心策略来进行动作的选择:\n","- 以$\\epsilon$的概率,随机选择动作\n","- 以$1-\\epsilon$的概率,选择贪心策略下最好的动作\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B00433C90D8D43DF853A5155365159DD","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## Q-Learning证明\n","### 确定环境的情况\n","我们来考虑一个马尔科夫决策过程，状态值函数$v(s)$和动作状态函数$q(s,a)$分别满足Bellman方程：\n","$v_{\\pi}(s) = \\sum\\limits_{\\alpha\\in\\mathcal{A}}\\pi(a|s)\\left(R_s^a+\\gamma\\sum\\limits_{s' \\in S}P^{a}_{ss'}v_{\\pi}(s')\\right)$\n","$q_{\\pi}(s,a) = R^a_s + \\gamma \\sum\\limits_{s'\\in S}P_{ss'}^a\\sum\\limits_{a'\\in A}\\pi(a'|s')q_{\\pi}(s',a')$\n","\t如果已知 $R_s^a$,$P^a_{ss'}$ ，我们面对的是一个model-based问题，理论上存在通过解Bellman方程的方法，来获得策略$\\pi$下的动作值函数。但是这样仍然有一个问题：解Bellman方程需要矩阵求逆，而这个操作对于稍微大一点的系统计算量太大，此路常常不通。\n","### 贪心策略\n","我们换一个思路，假设有一个最优的值函数，它是所有策略下对值函数最好的估计，同时，动作状态函数也是最优的。我们把能实现最优值函数的策略叫做策略$\\pi^*$ ，关于最优策略有一个定理：\n","1. 对于任意的马尔科夫决策过程，总会有一个最优策略 $\\pi^*$  比其余所有策略更好或者相等\n","2. 所有的最优策略都会让状态值函数取得最高\n","3. 所有的最优策略都会让动作值函数取得最高\n","\t这个定理的意思是最优策略就是让值函数最大化的策略，而且这个策略总会存在。其实这个定理比较好理解，值函数包含动作状态函数就是对状态或者状态+动作的收益的估计，让收益最大化的策略自然就是最好的策略。\n","\n","现在我们可以写下Bellman最优方程：\n","$v_{*}(s) = \\max\\limits_{a}\\left(R_s^a+\\gamma\\sum\\limits_{s' \\in S}P^{a}_{ss'}v_{*}(s')\\right)$\n","$q_{*}(s,a) = R^a_s + \\gamma \\sum\\limits_{s'\\in S}P_{ss'}^a\\max\\limits_{a'}q_{*}(s',a')$\n","虽然现在问题还没有解决，但是我们已经初步看到了Q-learning的算法基础-greedy policy选择动作。\n","### Value Iteration\n","只要我们解出Bellman最优方程，就可以获得RL问题的答案，然而我们Bellman最优方程很难解，我们尝试用iteration的方法来解Bellman方程。\n","Value iteration方法属于动态规划，将MDP问题分成两个子问题：\n","1.一个当前情况(s)下的最优动作\n","2.在后续状态(s')下沿着最优策略\n","继续进行最优性定理告诉我们：\n","一个策略$\\pi(a|s)$在状态s上取得最优值函数$v_{\\pi}(s) = v_*(s)$,当且仅当：对于从状态$s$可以到达的任何状态$s'$，$\\pi$从状态$s'$中能够获得最优值函数，$v_{\\pi}(s') = v_*(s')$。\n","根据Bellman optimal equation:\n","$v_{*}(s) = \\max\\limits_{a}\\left(R_s^a+\\gamma\\sum\\limits_{s' \\in S}P^{a}_{ss'}v_{*}(s')\\right)$\n","当状态s'的最优值函数已经找到，那么再往前看一步就是状态s的最优值函数。value iteration就是不断迭代Bellman optimal function。这里的直觉来自于Bellman optimal function的\"倒着推\"：\n","首先，终结状态的状态值函数是确定\n","然后，离终结状态的最优值函数可以确定\n","不断迭代，直到离终结状态最远的最优状态值函数被确定\n","### 压缩映射定理\n","现在我们试图来证明value iteration最终收敛到$v_*$。\n","根据压缩映射定理：\n","对于任何在算子 $T(v)$下完备(即封闭)的度量空间 $V$ ，如果算子 $T$ 为 $\\gamma$-压缩，那么会有：\n","* $T(v)$ 最终收敛到一个唯一的固定点$v^*$\n","* 线性收敛速度正比于$\\gamma$\n","这个定理可以这样理解，每次算子 $T(v)$作用到某个v(s)，都会\"压缩\"v(s)和$v^*$的距离，直到最终收敛到。现在就差算子 $T(v)$ :\n","事实上我们可以把Bellman optimal equation右边整体看成这个算子，也就是:\n","$T(v) = \\max\\limits_{a}R_s^a + \\gamma P^av$\n","\n","我们把这个算子叫做Bellman optimality backup operator，所以只要证明了Bellman optimality back operator是一个$\\gamma$ -压缩就行了。\n","那么什么是$\\gamma$ -压缩呢？就是我们称一个算子 $T$为$\\gamma$ -压缩，那么会有：\n","$\\|T(u)-T(v)\\|_{\\infty}\\leq\\gamma\\|u-v\\|_{\\infty}$,其中$\\gamma < 1$\n","而$\\|u-v\\|_{\\infty} = \\max\\limits_{s}|u(s)-v(s)|$表示两个值函数在任意状态上面的最大差距。\n","对于Bellman optimality backup operator:\n","\n","![Image Name](https://cdn.kesci.com/upload/image/qg4def1gei.png?imageView2/0/w/960/h/960)\n","又由Bell optimal equation，可得value iteration最终收敛到最优值函数。\n"]},{"cell_type":"markdown","metadata":{"id":"DB99B32DC7B144019663B8BF9250125E","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["### 结论的一般化\n","#### 定理 1\n","给定一个MDP，满足更新规则$Q_{t+1}(x_t,a_t) = (1-\\alpha_t(x_t,a_t))Q_t(x_t,a_t) + \\alpha_t(x_t,a_t)[r_t +\\gamma \\max\\limits_{b\\in A}Q_t(x_{t+1},b)]$， 那么它将收敛到最优的Q-function当且仅当\n","(1) $\\sum_t\\alpha_t(x,a)=\\infty$\n","(2) $\\sum_t\\alpha_t^2(x,a)<\\infty$\n","\n","#### 证明\n","不妨令$\\Delta_t(x,a) = Q_t(x,a) - Q^{*}(x,a)$\n","从而得到 $\\Delta_{t}(x_t,a_t) = (1-\\alpha_t(x_t,a_t))\\Delta_t(x_t,a_t) + \\alpha_t(x_t,a_t)[r_t +\\gamma \\max\\limits_{b\\in A}Q_t(x_{t+1},b) - Q^{*}(x_t,a_t)]$\n","定义一个函数$F_t(x,a) = r(x,a,X(x,a)) + \\gamma\\max\\limits_{b\\in A}Q_t(y,b) - Q^{*}(x,a)$\n","\n","那么 $E(F_t(x,a))=(TQ_t)(x,a)-Q^{*}(x,a) = (TQ_t)(x,a)-(TQ^{*})(x,a)$\n","通过上面block的公式，我们知道$E(F_t(x,a))\\leq \\gamma\\|\\Delta_t\\|$\n","最终我们得到$var(F_t(x)) = var[r(x,a,X(x,a))+\\gamma\\max\\limits_{b\\in A}Q_t(y,b)]\\leq C(1+\\|\\Delta_t\\|^2)$\n","这里C是一个常数\n","#### 引理\n","对一个随机过程$\\Delta_t$，满足$\\Delta_{t+1}(x) = (1-\\alpha_t(x))\\Delta_t(x)+\\alpha_t(x)F_t(x)$，那么它将收敛到0当且仅当\n","(1) $\\sum_t\\alpha_t(x,a)=\\infty$\n","(2) $\\sum_t\\alpha_t^2(x,a)<\\infty$\n","(3) $E(F_t(x))\\leq \\gamma\\|\\Delta_t\\|$\n","(4) $var(F_t(x))\\leq C(1 + \\|\\Delta_t\\|^2)$, C>0\n","\n","由此引理，我们就得到了Q-learning的收敛性。"]},{"cell_type":"markdown","metadata":{"id":"71A674F3B26C4A4E858648AAD808D19B","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## Q-Learning实战代码"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BD664A78753943BAB485F9AE35B9E80F","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["class Q_table(Base_Q_table):\n","    def __init__(self, length, height, gamma, actions=4, alpha=0.005):\n","        super().__init__(length, height, actions, alpha, gamma)\n","\n","    def update(self, a, s0, s1, r, is_terminated):\n","        #和前面两种方法区别最大的就是这里，区别在于Q Learning直接基于当前数据来更新，而非以前算出的数据\n","        #这里就是on-policy和off-policy的区别所在\n","        ########################################\n","        ## Programming 5: 更新table\n","        self.table[self._index(a,s0[0],s0[1])] += self.alpha*(r + self.gamma*self.max_q(s1[0],s1[1]) - self.table[self._index(a,s0[0],s0[1])])\n","        ########################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FEF3287A4AD546DF8F6403EA5A79F478","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","val = [0] * 150\n","x = [0] * 150\n","def cliff_walk():\n","    env = Env(length=12, height=4)\n","    table = Q_table(length=12, height=4, gamma=0.9)\n","    for num_episode in range(3000):\n","        episodic_reward = 0\n","        is_terminated = False\n","        s0 = [0, 0]\n","        while not is_terminated:\n","            action = table.take_action(s0[0], s0[1], num_episode)\n","            r, s1, is_terminated = env.step(action)\n","            table.update(action, s0, s1, r, is_terminated)\n","            episodic_reward += r\n","            s0 = s1\n","        if num_episode % 20 == 0:\n","            val[(int)(num_episode/20)] = episodic_reward\n","        env.reset()\n","cliff_walk()\n","for i in range((int)(3000/20)):\n","    x[i] = i\n","plt.plot(x, val, ls=\"-\", lw=2, label=\"plot figure\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"645A731D39214C5F8F3B560B7AEE7F74","jupyter":{},"mdEditEnable":false,"notebookId":"604f2dd7df26380015a09910","slideshow":{"slide_type":"slide"},"tags":[]},"source":["参考的训练过程如下：\n","\n","![Image Name](https://cdn.kesci.com/upload/image/qq0f54jk5e.png?imageView2/0/w/960/h/960)\n"]},{"cell_type":"markdown","metadata":{"id":"D101F767A7574A2EAB318C48D0109BF5","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## 下面是不同gamma对收敛率的影响"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5F6D203E8E0E49AA8A12DB85D5153348","jupyter":{},"scrolled":false,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["val = [0] * 150\n","x = [0] * 150\n","def cliff_walk1(ngamma):\n","    env = Env(length=12, height=4)\n","    table = Q_table(length=12, height=4, gamma=ngamma)\n","    for num_episode in range(3000):\n","        episodic_reward = 0\n","        is_terminated = False\n","        s0 = [0, 0]\n","        while not is_terminated:\n","            action = table.take_action(s0[0], s0[1], num_episode)\n","            r, s1, is_terminated = env.step(action)\n","            table.update(action, s0, s1, r, is_terminated)\n","            episodic_reward += r\n","            s0 = s1\n","        if num_episode % 20 == 0:\n","            val[(int)(num_episode/20)] = episodic_reward\n","        env.reset()\n","for i in range((int)(3000/20)):\n","    x[i] = i\n","cliff_walk1(0.1)\n","plt.plot(x, val, ls=\"-\", lw=2, label=\"gamma:0.1\")\n","cliff_walk1(0.4)\n","plt.plot(x, val, ls=\"-\", lw=2, label=\"gamma:0.4\")\n","cliff_walk1(0.7)\n","plt.plot(x, val, ls=\"-\", lw=2, label=\"gamma:0.7\")\n","cliff_walk1(0.95)\n","plt.plot(x, val, ls=\"-\", lw=2, label=\"gamma:0.95\")\n","cliff_walk1(1.0)\n","plt.plot(x, val, ls=\"-\", lw=2, label=\"gamma:1\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"8B74B31B9A3B42579B2DB7C86AC37B29","jupyter":{},"mdEditEnable":false,"notebookId":"604f2dd7df26380015a09910","slideshow":{"slide_type":"slide"},"tags":[]},"source":["**Ex2 可以看到不同的$\\gamma$对收敛的效果影响非常大，请思考其中的原因，并写出$\\gamma \\in [0,1]$过大或者过小所带来的影响。**\n","\n","$\\gamma$用于调节权重$Q(s,a)$衰减。当$\\gamma$过大，累加值收敛速度慢，模型学习收敛慢。反之当$\\gamma$过小，未来信息衰减较快，得到最终的收敛结果慢。"]},{"cell_type":"markdown","metadata":{"id":"08EE8D0E6E33474AA5A9CCF1C18E4FDE","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[]},"source":["# 总结\n","Sarsa和Q-Learning算法是强化学习中两个非常基础的算法，也是在实践过程中比较好用的算法。掌握这两个基础算法对将来进行深层次的学习很有帮助。不同的学习率$\\alpha$和超参数$\\gamma$对收敛速率的影响比较大，但如果学习率过高就可能无法达到最优解。$\\epsilon$-greedy算法中的$\\epsilon$也是非常重要的参量，对平衡探索和利用的关系非常重要。"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":1}
